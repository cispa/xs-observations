{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dde39b1-ba3d-458f-935c-fce25a5a4b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aca3a7-a9c6-4ec2-ab17-8b0cf1f8c375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ac69a9-24ce-45b4-8ad4-b91c7d2e9587",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import check_5_vals, stricter_pm, get_uniques, Conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48ea723-7541-45a4-9330-1b72bf4cce4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PGDATABASE\"] = \"dil_cf\"\n",
    "\n",
    "with open(\"../../database.env\") as f:\n",
    "    for line in f:\n",
    "        key, value = line.strip().split('=', 1)\n",
    "        os.environ[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f42ad1a-a09d-41c2-b2de-be434c0ec4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = Conn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1260f24-7fdc-4bec-91c8-6b937c9e1b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples = conn.select(\"SELECT * FROM site\")\n",
    "\n",
    "sites = pd.DataFrame(tuples, columns=[\"id\", \"rank\", \"site\", \"urls\", \"crawl_urls\", \"timeout_crawl\", \"error\", \"error_py\", \"crawled_urls\", \"after_basic\", \"after_trees\", \"after_trees_limit\", \"actual_urls\", \"insertion_time\", \"confirmed_urls\", \"timeout_dyn\", \"finished\"]).sort_values(\"rank\")\n",
    "\n",
    "display(sites.head())\n",
    "\n",
    "tuples = conn.select(\"SELECT * FROM accept\")\n",
    "accept = pd.DataFrame(tuples, columns=[\"id\", \"site\", \"rank\", \"browser\", \"version\", \"clicked_count\", \"clicked\", \"locator_count\", \"unique_locators\", \"locators\", \"cookies_before\", \"cookies_after\", \"cookies_new\", \"cookies_removed\", \"cookies_changed\", \"error\", \"insertion_time\"]).sort_values(\"rank\")\n",
    "display(accept.head())\n",
    "\n",
    "more = False\n",
    "if more:\n",
    "    tuples = conn.select(\"SELECT * FROM dyn_conf\")\n",
    "    dyn_conf = pd.DataFrame(tuples, columns=[\"id\", \"browser\", \"version\", \"site\", \"opg_url\", \"url\", \"inc_method\", \"state\", \"run\", \"observation\", \"error\", \"notes\", \"response\", \"insertion_time\"])\n",
    "    display(dyn_conf.head())\n",
    "\n",
    "    tuples = conn.select(\"SELECT * FROM responses\")\n",
    "    resp = pd.DataFrame(tuples, columns=[\"id\", \"site\", \"url\", \"state\", \"req_headers\", \"resp_code\", \"resp_headers\",\n",
    "                                           \"resp_body_hash\", \"resp_body_info\", \"frames\", \"error_text\",\n",
    "                                           \"insertion_time\"])\n",
    "    display(resp.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51826418-f792-4aa8-934b-eba00d924be9",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "## Tranco Top 10K\n",
    "\n",
    "1. **URL Collection**:\n",
    "    - Visit homepage (https://{site}/) wait until \"load\" (max: 30s) in Chromium\n",
    "    - Extract all HTTP(S) links\n",
    "    - Record all outgoing HTTP(S) requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800d3e01-f8c7-46d0-b720-9efe25d2916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some entries are duplicated (race condition in dil.js)\n",
    "# Only keep the last entry\n",
    "sites = sites.sort_values(\"id\")\n",
    "sites = sites.drop_duplicates(subset=\"rank\", keep=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2cd491-88de-47a9-99a3-46c9d627e260",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites[\"crawl_urls\"] = sites[\"crawl_urls\"].apply(lambda x: sorted(x))\n",
    "sites[\"crawled_urls\"] = sites[\"crawled_urls\"].apply(lambda x: sorted(x))\n",
    "sites[\"crawled_any\"] = sites[\"crawled_urls\"].str.len() != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f288256-fbaa-492d-bcdd-22bef3976a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview of table structure\n",
    "print(\"Overview:\")\n",
    "display(sites.head(2))\n",
    "\n",
    "\n",
    "# Errors on the tested sites (URL + response collection)\n",
    "print(\"Crawled sites:\")\n",
    "display(sites[\"crawled_any\"].value_counts())\n",
    "print(\"Errors on tested sites (crawled-any):\")\n",
    "#display(sites[[\"crawled_any\", \"error\"]].apply(lambda x: (x[\"crawled_any\"], x[\"error\"].split(\"at http\")[0].split(\"exceeded\")[0].split(\"Browser closed\")[0].split(\"{}\")[0].split(\"keys\")[0].split(\"sed!\\n==\")[0].split(\"hed!\\n==\")[0]), axis=1).value_counts().to_frame())\n",
    "display(sites[[\"crawled_any\", \"error\"]].apply(lambda x: (x[\"crawled_any\"], x[\"error\"].split(\"\\n\")[0].split(\" at \")[0]), axis=1).value_counts().to_frame())\n",
    "\n",
    "display(sites[\"error_py\"].value_counts().to_frame())\n",
    "\n",
    "# URLs collected:\n",
    "print(f\"URLs collected on: {sites.loc[sites['urls'].str.len() != 0].shape[0]} sites\")\n",
    "\n",
    "print(f\"URLs attempted to crawl on: {sites.loc[sites['crawl_urls'].str.len() != 0].shape[0]} sites\")\n",
    "\n",
    "# URLs crawled:\n",
    "print(f\"URLs crawled on: {sites.loc[sites['crawled_urls'].str.len() != 0].shape[0]} sites (the ones that are missing here crashed in collect_responses)\")\n",
    "\n",
    "# Same URLs crawled as tried:\n",
    "print(f\"All wanted URLs crawled on: {sites.loc[(sites['crawled_urls'] == sites['crawl_urls']) & (sites['crawled_urls'].str.len() != 0)].shape[0]} sites (either timeout or othe issue occured, e.g., crash in collect_responses)\")\n",
    "\n",
    "# Limit to crawled any sites\n",
    "sites_crawled = sites.loc[sites[\"crawled_any\"]]\n",
    "print(\"Timeouts:\")\n",
    "display(sites_crawled[[\"timeout_crawl\"]].value_counts().to_frame())\n",
    "display(sites_crawled[[\"timeout_dyn\"]].value_counts().to_frame())\n",
    "display(sites_crawled[[\"timeout_crawl\", \"timeout_dyn\"]].value_counts().to_frame())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f77dd3-72e5-4ea3-b914-97fa14faa614",
   "metadata": {},
   "source": [
    "2. **Response Collection**:\n",
    "    - Open three chromium instances\n",
    "    - Fresh state **_ano**, visited homepage state **_visited** + optional: \"accepted cookies\" state **_accepted**\n",
    "        - **_accepted** additional infos: \n",
    "            - visit homepage (wait until \"load\", max: 30s)\n",
    "            - wait 2s, save cookies\n",
    "            - locate potential cookie confirmation \"buttons\"\n",
    "                - e.g., 'button:has-text(\"Accept all cookies\")'\n",
    "                - list of ~100 locators\n",
    "                - try to locate them for 10s\n",
    "            - take screenshot: before\n",
    "            - try to click on all located locators\n",
    "                - one after the other\n",
    "                - most specific first (length of locator string)\n",
    "                - first hover them (2s), then click (2s)\n",
    "            - wait 2s, then take screenshot: after\n",
    "            - save cookies\n",
    "            - if at least one locator was clicked and cookies changed (either new, removed, or value changed) -> success\n",
    "    - Visit URLs in all states (2 or 3)\n",
    "        - on every URL wait until \"load\" (max: 30s) (top-level request)\n",
    "        - max 500 URLs (if more than 500 exist, random selection of all recorded URLs)\n",
    "        - max 1 hour\n",
    "        - Record traffic/responses (with playwright; does not record everything for errors and similar; other option would be HAR or proxy?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae31711-7869-42e2-b2fc-214c9c3f48cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(row):\n",
    "    ll = row[\"urls\"]\n",
    "    links = []\n",
    "    requests = []\n",
    "    total = len(ll)\n",
    "    for l in ll:\n",
    "        if l[\"link\"]:\n",
    "            links.append(l)\n",
    "        if l[\"request\"]:\n",
    "            requests.append(l)\n",
    "    return {\"Links\": len(links), \"Requests\": len(requests), \"Total\": total}\n",
    "print(\"URLs collected stats:\")\n",
    "display(sites.loc[sites['crawl_urls'].str.len() != 0].apply(count, result_type=\"expand\", axis=1).describe())\n",
    "display(sites.loc[sites['crawl_urls'].str.len() != 0].apply(count, result_type=\"expand\", axis=1).sum())\n",
    "\n",
    "print(\"URLs crawled:\")\n",
    "display(sites.loc[sites['crawled_urls'].str.len() != 0].apply(count, result_type=\"expand\", axis=1).describe())\n",
    "display(sites.loc[sites['crawled_urls'].str.len() != 0].apply(count, result_type=\"expand\", axis=1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff7ecf9-e1ed-411b-85df-5822660f0a46",
   "metadata": {},
   "source": [
    "3. **Pruning**:\n",
    "    - Get all traffic data for all crawled URLs\n",
    "    - Fit response data to trees:\n",
    "        - Status-Code\n",
    "        - smoothed (Security)-Headers: \"content-type\", \"x-frame-options\", \"location\", \"content-disposition\", \"x-content-type-options\", \"cross-origin-opener-policy\", \"cross-origin-resource-policy\", \"content-security-policy\"\n",
    "        - body type: e.g., HTML, img, ... (inferred with `file` command)\n",
    "    - Basic pruning: only keep URLs that have at least one attribute with more than one recorded value\n",
    "    - Advanced pruning:\n",
    "        - All Chromium and Firefox trees\n",
    "        - Predict the outcome of every tree for every remaining URL-state pair\n",
    "        - For every tree with at least two different predictions for a URL -> add URL-inclusion method to set of to confirm URLs\n",
    "            - special cases for some trees (e.g., img-height):\n",
    "                - even if all predictions are the same, they might be distinguishable (artifact of the smoothing)\n",
    "                - if all predictions are positive (e.g, height=50), compare other property (e.g., bodyhash) and if that property differs -> add to set\n",
    "            - example:     `urls = {\"img\": {\"https://google.com/\": \"cfw\"}, \"https://google.com/search/\": \"c\"}, \"iframe\": {\"https://google.com/\": \"f\"}}`; every inc-url pair is tested in both browsers regardless of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c0fb9e-2855-49cb-8856-a4b22c4a255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sites crawled\n",
    "sc = sites_crawled\n",
    "def get_urls(dat):\n",
    "    url_set = set()\n",
    "    url_list = []\n",
    "    for inc, entry in dat.items():\n",
    "        for url, browser in entry.items():\n",
    "            url_list.append(url)\n",
    "            url_set.add(url)\n",
    "    return url_list, url_set\n",
    "\n",
    "def count_pruning(row):\n",
    "    pc = row[\"crawl_urls\"]\n",
    "    c = row[\"crawled_urls\"]\n",
    "    ab = row[\"after_basic\"]\n",
    "    at = row[\"after_trees\"]\n",
    "    atl = row[\"after_trees_limit\"]\n",
    "    act = row[\"actual_urls\"]\n",
    "    ul_at, us_at = get_urls(at)\n",
    "    ul_atl, us_atl = get_urls(atl)\n",
    "    ul_act, us_act = get_urls(act)\n",
    "                \n",
    "    return {\"crawl_urls\": len(pc), \"crawled_urls\": len(c), \"after basic\": len(ab), \"after trees (total inc-url pairs)\": len(ul_at), \"after trees (unique urls)\": len(us_at), \"after trees limit (total inc-url pairs)\": len(ul_atl), \"after trees limit (unique urls)\": len(us_atl), \"actual URLs\": len(ul_act), \"actual URLs (unique)\": len(us_act)}\n",
    "# The data describes it without browsers!\n",
    "print(\"Pruning stats:\")\n",
    "display(sc[[\"crawl_urls\", \"crawled_urls\", \"after_basic\", \"after_trees\", \"after_trees_limit\", \"actual_urls\"]].apply(count_pruning, axis=1, result_type=\"expand\").describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0685dd5-cd86-4f51-a940-af80863b6564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which inclusion methods are predicted?\n",
    "# Sites/URLs\n",
    "def collect_incs(row):\n",
    "    row = row[\"after_trees\"]\n",
    "    res = {\"any\": {}}\n",
    "    for inc in row.keys():\n",
    "        for url, browser_str in row[inc].items():\n",
    "            entry = res[\"any\"].get(inc, 0)\n",
    "            entry += 1\n",
    "            res[\"any\"][inc] = entry\n",
    "    return res\n",
    "met = sc[[\"after_trees\"]].apply(collect_incs, axis=1, result_type=\"expand\")\n",
    "met_any = pd.json_normalize(met[\"any\"]).agg([\"count\", \"sum\"]).T\n",
    "met_any[[\"count\", \"sum\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a7cc1f-cb8a-4e3b-a3e6-a1f37ad9e1ea",
   "metadata": {},
   "source": [
    "4. **\"Dynamic confirmation**:\n",
    "    - Test all remaining inclusion_method-url-browser pairs\n",
    "        - max 25 URLs for one inclusion method\n",
    "        - max 3h\n",
    "    - Test all possible states (regardless of whether the prediction was only for one state-pair)\n",
    "    - Prepare states: \n",
    "        - Same as in **response collection**\n",
    "        - Additionally for Firefox\n",
    "    - For every inc method:\n",
    "        - For every URL:\n",
    "            - For every browser; If browser should be tested:\n",
    "                - For every state:\n",
    "                    - wait 1s\n",
    "                    - visit `http://observer.org/opg/<inc>/?url=<url>`\n",
    "                    - wait until \"networkidle\", max: 30s; for window.open wait for \"networkidle\" or \"domcontentloaded\" of the new window\n",
    "                    - wait another 750ms (2000ms)\n",
    "                    - extract observations\n",
    "                    - (record responses)\n",
    "                - If observations for every state are the same -> remove browser from to_test list\n",
    "         - Repeat up to 5 times\n",
    "     - Get confirmed distinguishable pairs:\n",
    "         - 5 times different observations for one observation method -> confirmed browser-inc_method-url-state_a-state_b(-observation_method) pair\n",
    "         - additional sanity checking: \n",
    "             - the same observation is not allowed to be present in both states (e.g., random frame counts: [(0, 1), (0, 1), (1, 0), (1, 0), (0, 1)] -> not a confirmed pair)\n",
    "             - additional constraints for some methods: \n",
    "                 - e.g., custom code for postMessage, frame_count\n",
    "                 - heuristic: at least one value should occur two times for the same state?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a595c8fc-a5f6-4142-91bc-db47be910186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timout of dynamic sites\n",
    "sites_dyn = sites.loc[sites[\"actual_urls\"] != {}]\n",
    "display(len(sites_dyn))\n",
    "display(sites_dyn[\"timeout_dyn\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cb63fa-8a4f-4540-9122-23748ae77c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early abort stats\n",
    "dyn_conf_run = pd.DataFrame(conn.select(\"SELECT run, COUNT(id) from dyn_conf GROUP BY run\"))\n",
    "dyn_conf_run.loc[\"sum\"] = dyn_conf_run.sum()\n",
    "display(dyn_conf_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86379b8e-0953-465d-afc3-a7ccae36f902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total time taken\n",
    "sites[\"insertion_time\"].max() - sites[\"insertion_time\"].min() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296b19cf-8fca-4e09-b472-2892cbff2904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data (one entry for every confirmed URL)\n",
    "conf = sc.loc[sc[\"confirmed_urls\"].str.len() != 0]\n",
    "confs_raw = pd.DataFrame()\n",
    "for row in conf[[\"confirmed_urls\", \"site\"]].iterrows():\n",
    "    row = row[1]\n",
    "    site = row[\"site\"]\n",
    "    for state, df in row[\"confirmed_urls\"].items():\n",
    "        new = pd.DataFrame.from_dict(df)\n",
    "        new[\"site\"] = site\n",
    "        new[\"state\"] = state\n",
    "        new = new.rename(columns={\"0\": \"observation_methods\"})\n",
    "        confs_raw = pd.concat([confs_raw, new])\n",
    "display(confs_raw.head())\n",
    "\n",
    "confs_raw[\"observation_methods\"] = confs_raw[\"observation_methods\"].apply(sorted)\n",
    "from publicsuffix2 import get_sld\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "confs_raw[\"real_site\"] = confs_raw[\"url\"].apply(lambda x: get_sld(urlparse(json.loads(x)).hostname))\n",
    "confs_raw[\"same_site\"] = confs_raw[\"site\"] == confs_raw[\"real_site\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d90cde5-ae65-4bd1-8f09-5c655f0f4dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only sites that distinguish visited from ano\n",
    "confs_ano = confs_raw.loc[~confs_raw[\"state\"].str.contains(\"acc\")]\n",
    "# Only sites that either distinguish ano from accepted or visited from accepted\n",
    "confs_acc = confs_raw.loc[confs_raw[\"state\"].str.contains(\"acc\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cba904-ac0b-46ef-9162-4dda4b5816e0",
   "metadata": {},
   "source": [
    "## History sniffing results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac2b225-2c83-41ae-ac2f-36e28aa266d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "confs = confs_ano\n",
    "# Confs same-site only\n",
    "confs = confs.loc[confs[\"same_site\"] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a293388b-f4d0-491d-a688-fbe18fafedc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique sites\", confs[\"site\"].nunique())\n",
    "print(confs[\"observation_methods\"].value_counts().to_frame().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d430e9c6-eb4f-4ca0-a3f8-6acfa5d4b951",
   "metadata": {},
   "source": [
    "### pM and co. check heuristics\n",
    "- check heuristics implemented in dil.py\n",
    "- mostly good\n",
    "- by using the additional heuristic: at least one state is not allowed to have 5 different values, for most methods almost nothing is lost\n",
    "- only is noisy el-message (16738): additional restriction, one state is only allowed to have one value!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e259b7b8-a2fb-4001-a695-61507a5bad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "confs_5 = confs_raw[~confs_raw.apply(check_5_vals, axis=1)]\n",
    "confs_5[\"observation_methods\"] = confs_5[\"observation_methods\"].astype(str)\n",
    "print(f\"Methods that have entries with 5 unique observations in both vals_a and vals_b: {confs_5['observation_methods'].unique()}\")\n",
    "display(confs_5.groupby(\"observation_methods\").count())\n",
    "with pd.option_context(\"display.max_colwidth\", 1000):\n",
    "    display(confs_5[[\"observation_methods\", \"vals_a\", \"vals_b\"]].drop_duplicates(subset=\"observation_methods\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987db179-0027-4202-a90c-f29c7fd0fd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to stricter heuristic for analysis!\n",
    "confs_heuristic = confs[confs.apply(check_5_vals, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dff214d-ed02-411a-b421-b76da4ee0912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "import diff_match_patch as dmp_module\n",
    "from IPython.core.display import HTML\n",
    "dmp = dmp_module.diff_match_patch()\n",
    "\n",
    "for method in [\"['contentDocument']\", \"['events-fired-all', 'events-fired']\", \"['length']\", \"['performanceAPI.smooth']\", \"['el-message']\"]:\n",
    "#for method in [\"['el-securitypolicyviolation']\", \"['fetch_response']\", \"['el-blur']\", \"['history.length']\", \"['el-error']\"]:\n",
    "    def get_diff(row):\n",
    "        vals_a = str(row[\"vals_a\"])[:5000]\n",
    "        vals_b = str(row[\"vals_b\"])[:5000]\n",
    "        diff = dmp.diff_main(vals_a, vals_b)\n",
    "        return row[\"site\"], row[\"url\"][:100], row[\"inc_method\"], dmp.diff_prettyHtml(diff)\n",
    "\n",
    "    with pd.option_context(\"display.max_colwidth\", 100):\n",
    "        for (conf, name) in [(confs_5, \"5 values\"), (confs_heuristic, \"less than 5 values\")]:\n",
    "            conf = conf.copy()\n",
    "            conf.loc[:, \"observation_methods\"] = conf.loc[:, \"observation_methods\"].astype(str)\n",
    "            print(name, method)\n",
    "            display(HTML(conf.loc[conf[\"observation_methods\"] == method].iloc[:5].apply(get_diff, axis=1, result_type=\"expand\").to_html(escape=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcd1537-2e06-4899-9e17-9e6f591677fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even with strict (!=5) heuristic, pM still contains many FP likely values\n",
    "with pd.option_context(\"display.max_colwidth\", 1000):\n",
    "    pm = confs_heuristic.loc[confs_heuristic[\"observation_methods\"].apply(str) == \"['el-message']\"]\n",
    "    display(pm[[\"vals_a\", \"vals_b\"]].head(3))\n",
    "    pm = pm.loc[pm.apply(stricter_pm, axis=1)]\n",
    "    display(pm[[\"vals_a\", \"vals_b\"]].head(3))\n",
    "    \n",
    "# Use even stricter heuristic for pMs:\n",
    "# one state is only allowed to have a maximum of one observation, this leads to some FNs, but should remove all FPs\n",
    "\n",
    "confs_heuristic = confs_heuristic.loc[confs_heuristic.apply(stricter_pm, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7d8ec9-fb92-4ad0-be94-cf85e8bbb81e",
   "metadata": {},
   "source": [
    "## Continue main history sniffing analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb58ca0f-c285-4e23-abe1-ce0b7e184414",
   "metadata": {},
   "outputs": [],
   "source": [
    "confs_heuristic[\"channel\"] = confs_heuristic[\"inc_method\"] + \"-\" +  confs_heuristic[\"observation_methods\"].apply(str)\n",
    "display(confs_heuristic[\"site\"].nunique())\n",
    "display(confs_heuristic.groupby([\"site\"])[\"browser\"].unique().apply(sorted).astype(str).to_frame().value_counts().to_frame())\n",
    "display(confs_heuristic.groupby([\"browser\", \"state\"])[\"site\"].nunique().to_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3498608-a51b-4817-ab6d-111c625a96e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the observation methods to have one row for every observation method\n",
    "\n",
    "\n",
    "c_exp = confs_heuristic.explode(\"observation_methods\")\n",
    "c_exp[\"channel\"] = c_exp[\"inc_method\"] + \"-\" + c_exp[\"observation_methods\"]\n",
    "# Appy pM Heuristic again\n",
    "c_exp = c_exp.loc[c_exp.apply(stricter_pm, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27ef105-3609-466f-bfa7-1554e549bb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average URLs/site\n",
    "display(c_exp.groupby(\"site\")[\"url\"].nunique().to_frame().describe())\n",
    "# Average inc-url-pairs/site\n",
    "display(c_exp.groupby(\"site\")[\"opg_url\"].nunique().to_frame().describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ffe1d5-8a55-4cab-9280-dcf29b4e91f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of vulnerabel sites\n",
    "c_exp[\"site\"].nunique()/len(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceec2fbe-1d9c-45d1-8c1e-db01d73348c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_exp_c = c_exp.loc[c_exp[\"browser\"] == '\"chromium\"']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2702571-bd6c-4655-ac24-769d7af324b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fancy tables with Sites both browser, only chromium, only firefox, (sorted by sum)\n",
    "browser_data = {}\n",
    "\n",
    "for grouping, name in [([\"inc_method\"], \"incs\"), ([\"observation_methods\"], \"methods\"), ([\"inc_method\", \"observation_methods\"], \"channels\")]:\n",
    "    df = c_exp.loc[c_exp[\"observation_methods\"] != \"events-fired-all\"].groupby(grouping).apply(get_uniques).apply(pd.Series).sort_values(\"Sum\", ascending=False)\n",
    "    df = df.reset_index().rename(columns={\"inc_method\": \"Inclusion Method\", \"observation_methods\": \"Observation Method\"})\n",
    "    if name == \"incs\":\n",
    "        df[\"Inclusion Method\"] = df[\"Inclusion Method\"].apply(json.loads)\n",
    "        df = df.set_index([\"Inclusion Method\"])\n",
    "    elif name == \"methods\":\n",
    "        df = df.set_index([\"Observation Method\"])\n",
    "    else:\n",
    "        df[\"Inclusion Method\"] = df[\"Inclusion Method\"].apply(json.loads)\n",
    "        df = df.set_index([\"Inclusion Method\", \"Observation Method\"])\n",
    "    df = df.rename(index={\"fetch_response\": \"fetch-response\"})\n",
    "    browser_data[name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaa3a12-f6e2-4039-997a-34e90ac92a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for name in [\"incs\", \"methods\", \"channels\"]:\n",
    "for name in [\"channels\"]:\n",
    "    df = browser_data[name][[\"Both\", \"Only C\", \"Only FF\", \"Sum\"]].head(20)\n",
    "    df.index = pd.MultiIndex.from_tuples([(x[0], x[1].replace('.smooth', '')) for x in df.index]).set_names(['Inclusion Method', 'Observation Method'])\n",
    "    \n",
    "    df.columns = pd.MultiIndex.from_arrays([[\"Vulnerable sites\", \"Vulnerable sites\", \"Vulnerable sites\", \"Vulnerable sites\"], [\"Both\", \"Only Chromium\", \"Only Firefox\", \"Sum\"]])\n",
    "    display(df)\n",
    "    latex_table = df.style.to_latex(hrules=True, multicol_align=\"c\")\n",
    "    print(latex_table)\n",
    "    with open(f\"res/paper_history_{name}.tex\", \"w\") as f:\n",
    "        f.write(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de518e47-9b71-416d-9536-b2b0eeb8dd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in [\"channels\"]:\n",
    "    df = browser_data[name][[\"Both\", \"Only C\", \"Only FF\", \"Sum\"]]\n",
    "    df.index = pd.MultiIndex.from_tuples([(x[0], x[1].replace('.smooth', '')) for x in df.index]).set_names(['Inclusion Method', 'Observation Method'])\n",
    "    \n",
    "    df.columns = pd.MultiIndex.from_arrays([[\"Vulnerable sites\", \"Vulnerable sites\", \"Vulnerable sites\", \"Vulnerable sites\"], [\"Both\", \"Only Chromium\", \"Only Firefox\", \"Sum\"]])\n",
    "    display(df)\n",
    "    df.to_csv(f\"res/paper_history_{name}_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0c4718-e2d7-4eba-83db-674b6774fc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(df.loc[df[('Vulnerable sites', 'Only Chromium')] > 0])} working channels in chrome visited\")\n",
    "print(f\"{len(df.loc[df[('Vulnerable sites', 'Only Firefox')] > 0])} working channels in firefox visited\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b452e2-afa8-4672-b64f-d9bda75a0b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context(\"display.max_rows\", 84):\n",
    "    display(browser_data[\"channels\"][[\"Both\", \"Only C\", \"Only FF\", \"Sum\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc6fbb7-9e09-471a-a4f7-00782e6a24f0",
   "metadata": {},
   "source": [
    "### Investigation of differences in browsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fdf330-44e8-4a66-b905-afdae0b70051",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_w = c_exp.loc[(c_exp[\"browser\"] == '\"chromium\"') & (c_exp[\"inc_method\"] == '\"window.open\"')][\"site\"].unique()\n",
    "f_w = c_exp.loc[(c_exp[\"browser\"] == '\"firefox\"') & (c_exp[\"inc_method\"] == '\"window.open\"')][\"site\"].unique()\n",
    "c_nw = c_exp.loc[(c_exp[\"browser\"] == '\"chromium\"') & (c_exp[\"inc_method\"] != '\"window.open\"')][\"site\"].unique()\n",
    "f_nw = c_exp.loc[(c_exp[\"browser\"] == '\"firefox\"') & (c_exp[\"inc_method\"] != '\"window.open\"')][\"site\"].unique()\n",
    "\n",
    "# Sites only having window.open in Chromium\n",
    "c_ow = set(c_w) - set(c_nw)\n",
    "# Sites only having window.open in Chromium and at least one non-window.open in Firefox\n",
    "print(len(c_ow & set(f_nw)))\n",
    "\n",
    "# Sites only having window.open in one of both\n",
    "w_only_one = set(c_w) ^ set(f_w)\n",
    "print(len(w_only_one))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb9038c-4050-4bb2-af81-294d1cea0760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many sites only vuln in Firefox (non-window.open) use SameSite None without Secure (e.g., manderson.org, walmart.ca)!\n",
    "# some of these site use https://experienceleague.adobe.com/docs/experience-platform/tags/client-side/satellite-object.html?lang=en#cookie-set or similar to set the cookies in javascript\n",
    "# they do not set SameSite nor Secure, Chromimu defaults to Lax, Firefox defaults to None and warns about Secure not being set\n",
    "\n",
    "browser_data[\"incs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b013c7-feb4-4cd4-919f-407f270b313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wo_len = c_exp.loc[(c_exp[\"observation_methods\"] == \"length\") & (c_exp[\"inc_method\"] == '\"window.open\"')]\n",
    "wo_len[\"a\"] = wo_len[\"vals_a\"].apply(lambda x: x[\"length\"]).apply(sorted).astype(str)\n",
    "wo_len[\"b\"] = wo_len[\"vals_b\"].apply(lambda x: x[\"length\"]).apply(sorted).astype(str)\n",
    "wo_len_only_one = wo_len.loc[wo_len[\"site\"].isin(w_only_one)]\n",
    "display(wo_len[[\"a\", \"b\"]].value_counts().head(10).to_frame())\n",
    "display(wo_len_only_one[[\"a\", \"b\"]].value_counts().head(10).to_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127f8d70-29cd-412b-84f6-fe0d716479d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firefox double-script and style-import\n",
    "# They all work due to 'timing-allow-origin' which was not in the response space\n",
    "\n",
    "# Img\n",
    "# 1 Site only working in Firefox has strange endless redirect in Chromium (thus not setting cookies)\n",
    "# Sites only working in Chromium, Chromium gets blocked by WAF/bot detection for some reason, Firefox does not\n",
    "\n",
    "# Iframe history length\n",
    "# client side redirects?\n",
    "\n",
    "# Iframe-dircsp\n",
    "# probably lax?, + all kind of stuff\n",
    "\n",
    "# el-securitypolicyviolation and not iframe-csp\n",
    "# frame-ancestors bug in Firefox\n",
    "\n",
    "\n",
    "with pd.option_context(\"display.max_colwidth\", 500):\n",
    "    #display(confs_heuristic.loc[(confs_heuristic[\"browser\"] == '\"firefox\"') & (confs_heuristic[\"inc_method\"] == '\"double-script\"')])\n",
    "    #display(confs_heuristic.loc[(confs_heuristic[\"browser\"] == '\"firefox\"') & (confs_heuristic[\"inc_method\"] == '\"style-import\"')])\n",
    "    #display(confs_heuristic.loc[(confs_heuristic[\"inc_method\"] == '\"img\"')].drop_duplicates(subset=[\"browser\", \"site\"]))\n",
    "    #display(c_exp.loc[(c_exp[\"inc_method\"] == '\"iframe\"') & (c_exp[\"observation_methods\"] == \"history.length\")].drop_duplicates(subset=[\"browser\", \"site\"]))\n",
    "    #display(c_exp.loc[(c_exp[\"inc_method\"] == '\"iframe-dircsp\"')].drop_duplicates(subset=[\"browser\", \"site\"]))\n",
    "    #display(c_exp.loc[(c_exp[\"observation_methods\"] == \"sheet\")].drop_duplicates(subset=[\"browser\", \"site\"]))\n",
    "    display(c_exp.loc[(c_exp[\"observation_methods\"] == \"el-securitypolicyviolation\") & (c_exp[\"inc_method\"] != '\"iframe-csp\"')].drop_duplicates(subset=[\"browser\", \"site\"]))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ce3805-0b9e-4c2d-9219-f041716a322a",
   "metadata": {},
   "source": [
    "## Cookie acceptance sniffing\n",
    "- Chromimum only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c921777d-9218-4ba9-a462-07ac4d041ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chromium x2 + Firefox\n",
    "ac = accept\n",
    "# Worked: clicked + changes\n",
    "worked = ac.loc[(ac[\"clicked_count\"] != 0) & ((ac[\"cookies_new\"].str.len() != 0) | (ac[\"cookies_removed\"].str.len() != 0) | (ac[\"cookies_changed\"].str.len() != 0))]\n",
    "# Did not work: clicked + no changes\n",
    "dnw = ac.loc[(ac[\"clicked_count\"] != 0) & ((ac[\"cookies_new\"].str.len() == 0) & (ac[\"cookies_removed\"].str.len() == 0) & (ac[\"cookies_changed\"].str.len() == 0))]\n",
    "# Changed without click\n",
    "cnc = ac.loc[(ac[\"clicked_count\"] == 0) & ((ac[\"cookies_new\"].str.len() != 0) | (ac[\"cookies_removed\"].str.len() != 0) | (ac[\"cookies_changed\"].str.len() != 0))]\n",
    "# No click no change\n",
    "ncnc = ac.loc[(ac[\"clicked_count\"] == 0) & ((ac[\"cookies_new\"].str.len() == 0) & (ac[\"cookies_removed\"].str.len() == 0) & (ac[\"cookies_changed\"].str.len() == 0))]\n",
    "\n",
    "for name, df in [(\"worked\", worked), (\"DNW\", dnw), (\"CNC\", cnc), (\"NCNC\", ncnc)]:\n",
    "    print(name)\n",
    "    display(df.groupby(\"browser\")[\"site\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ff69a9-35b2-46c8-bfc2-e581221f9e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Chromium accept crawl\n",
    "ac = accept.loc[accept[\"browser\"] == \"chromium\"].sort_values(\"insertion_time\").drop_duplicates(subset=\"site\").sort_values(\"rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbecd49-a504-418c-9dc0-620b41ef4e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worked: clicked + cookies changed\n",
    "worked = ac.loc[(ac[\"clicked_count\"] != 0) & ((ac[\"cookies_new\"].str.len() != 0) | (ac[\"cookies_removed\"].str.len() != 0) | (ac[\"cookies_changed\"].str.len() != 0))]\n",
    "# Did not work: clicked + no changes\n",
    "dnw = ac.loc[(ac[\"clicked_count\"] != 0) & ((ac[\"cookies_new\"].str.len() == 0) & (ac[\"cookies_removed\"].str.len() == 0) & (ac[\"cookies_changed\"].str.len() == 0))]\n",
    "# Changed without click\n",
    "cnc = ac.loc[(ac[\"clicked_count\"] == 0) & ((ac[\"cookies_new\"].str.len() != 0) | (ac[\"cookies_removed\"].str.len() != 0) | (ac[\"cookies_changed\"].str.len() != 0))]\n",
    "# No click no change\n",
    "ncnc = ac.loc[(ac[\"clicked_count\"] == 0) & ((ac[\"cookies_new\"].str.len() == 0) & (ac[\"cookies_removed\"].str.len() == 0) & (ac[\"cookies_changed\"].str.len() == 0))]\n",
    "print(\"Accept cookies module stats:\")\n",
    "print(f\"Worked: clicked + cookies changed on {len(worked)} sites\\nDid not work: clicked + no changes on {len(dnw)} sites\\nChanged without click on: {len(cnc)} sites\\nNo change no click on: {len(ncnc)} sites\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36da9da-c6e5-4e1a-a01a-52ae5b1696be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Chromium accept crawl (for dyn confirm)\n",
    "# Only use sites that were attempted to be crawled?!\n",
    "ac = accept.loc[accept[\"browser\"] == \"chromium\"].sort_values(\"insertion_time\").drop_duplicates(subset=\"site\", keep=\"last\").sort_values(\"rank\")\n",
    "ac = ac.loc[ac[\"site\"].isin(sites_dyn[\"site\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f45243d-42d6-4cb8-99e2-9ef552c29de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worked: clicked + cookies changed\n",
    "worked = ac.loc[(ac[\"clicked_count\"] != 0) & ((ac[\"cookies_new\"].str.len() != 0) | (ac[\"cookies_removed\"].str.len() != 0) | (ac[\"cookies_changed\"].str.len() != 0))]\n",
    "# Did not work: clicked + no changes\n",
    "dnw = ac.loc[(ac[\"clicked_count\"] != 0) & ((ac[\"cookies_new\"].str.len() == 0) & (ac[\"cookies_removed\"].str.len() == 0) & (ac[\"cookies_changed\"].str.len() == 0))]\n",
    "# Changed without click\n",
    "cnc = ac.loc[(ac[\"clicked_count\"] == 0) & ((ac[\"cookies_new\"].str.len() != 0) | (ac[\"cookies_removed\"].str.len() != 0) | (ac[\"cookies_changed\"].str.len() != 0))]\n",
    "# No click no change\n",
    "ncnc = ac.loc[(ac[\"clicked_count\"] == 0) & ((ac[\"cookies_new\"].str.len() == 0) & (ac[\"cookies_removed\"].str.len() == 0) & (ac[\"cookies_changed\"].str.len() == 0))]\n",
    "print(\"Accept cookies module stats:\")\n",
    "print(f\"Worked: clicked + cookies changed on {len(worked)} sites\\nDid not work: clicked + no changes on {len(dnw)} sites\\nChanged without click on: {len(cnc)} sites\\nNo change no click on: {len(ncnc)} sites\")\n",
    "print(f\"Locator found but no click on {len(ac.loc[(ac['clicked_count'] == 0) & (ac['locator_count'] != 0)])} sites (subset of two previous ones)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a99c73-089f-4456-8492-1e241a1355fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparative data (history sniffing, only chromium, strict heuristics, accepted worked)\n",
    "c_exp_comp = c_exp_c.loc[c_exp_c[\"site\"].isin(worked[\"site\"].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48085563-38f8-4c7e-ad33-0a6322cb2009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply strict heuristics!\n",
    "cookie = confs_acc[confs_acc.apply(check_5_vals, axis=1)]\n",
    "co_exp = cookie.explode(\"observation_methods\")\n",
    "co_exp[\"channel\"] = co_exp[\"inc_method\"] + \"-\" + co_exp[\"observation_methods\"]\n",
    "# Appy pM Heuristic\n",
    "co_exp = co_exp.loc[co_exp.apply(stricter_pm, axis=1)]\n",
    "# Only chromium\n",
    "co_exp = co_exp.loc[co_exp[\"browser\"] == '\"chromium\"']\n",
    "# Only first-party URLs\n",
    "co_exp = co_exp.loc[co_exp[\"same_site\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5043b3ba-e72c-4980-839d-b32775c95ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of vulnerable sites\n",
    "co_exp[\"site\"].nunique()/len(worked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8066acf-fb14-423e-a786-ad7375a00743",
   "metadata": {},
   "outputs": [],
   "source": [
    "co_hist = pd.concat([c_exp_comp, co_exp])\n",
    "display(co_hist.groupby(\"site\")[\"state\"].unique().apply(sorted).value_counts().to_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941c32b6-604a-4ad9-a7df-7d20578623e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib_venn import venn3, venn3_circles, venn3_unweighted\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "venn3_unweighted(subsets = (20, 28, 701, 12, 203, 10, 123),\n",
    "      set_labels = ('Accepted-Anonymous', 'Accepted-Visited', 'Visited-Anonymous'), alpha = 0.5,\n",
    "      subset_areas = (20, 28, 701 - 500, 12, 203 - 100, 10 + 50, 123 - 50))\n",
    "plt.savefig('res/venn_states.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed471d32-a43c-490e-9e5b-7be4469d2357",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Attempted sites\", len(worked))\n",
    "print(\"Cookie accepted sites\", co_exp[\"site\"].nunique())\n",
    "print(\"History comparative data\", c_exp_comp[\"site\"].nunique())\n",
    "print(\"Sites vuln in both\", len(set(co_exp[\"site\"].unique()) & set(c_exp_comp[\"site\"].unique())))\n",
    "print(\"Sites vuln only to cookie\", len(set(co_exp[\"site\"].unique()) - set(c_exp_comp[\"site\"].unique())))\n",
    "print(\"Sites vuln only to hist\", len(set(c_exp_comp[\"site\"].unique()) - set(co_exp[\"site\"].unique())))\n",
    "print(set(c_exp_comp[\"site\"].unique()) - set(co_exp[\"site\"].unique()))\n",
    "\n",
    "display(co_exp.groupby([\"browser\", \"state\"])[\"site\"].nunique().to_frame())\n",
    "display(co_exp.groupby(\"site\")[\"state\"].unique().apply(sorted).value_counts().to_frame())\n",
    "display(co_exp.groupby(\"url\")[\"state\"].unique().apply(sorted).value_counts().to_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeabdc8-dfc4-4261-aa71-539a61db4af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Channel and co analysis!\n",
    "browser_data = {}\n",
    "\n",
    "for grouping, name in [([\"inc_method\"], \"incs\"), ([\"observation_methods\"], \"methods\"), ([\"inc_method\", \"observation_methods\"], \"channels\")]:\n",
    "    df = co_hist.loc[co_hist[\"observation_methods\"] != \"events-fired-all\"].groupby(grouping).apply(get_uniques, cat=\"state\").apply(pd.Series).sort_values(\"Sum\", ascending=False)\n",
    "    df = df.reset_index().rename(columns={\"inc_method\": \"Inclusion Method\", \"observation_methods\": \"Observation Method\"})\n",
    "    if name == \"incs\":\n",
    "        df[\"Inclusion Method\"] = df[\"Inclusion Method\"].apply(json.loads)\n",
    "        df = df.set_index([\"Inclusion Method\"])\n",
    "    elif name == \"methods\":\n",
    "        df = df.set_index([\"Observation Method\"])\n",
    "    else:\n",
    "        df[\"Inclusion Method\"] = df[\"Inclusion Method\"].apply(json.loads)\n",
    "        df = df.set_index([\"Inclusion Method\", \"Observation Method\"])\n",
    "    df = df.rename(index={\"fetch_response\": \"fetch-response\"})\n",
    "    browser_data[name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e900af-c0ed-4262-9129-2c05c2d50029",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for name in [\"incs\", \"methods\", \"channels\"]:\n",
    "for name in [\"channels\"]:\n",
    "    df = browser_data[name][[\"Both\", \"Only Cookie\"]].head(20)# \"Only Hist\", \"Sum\"]].head(20)\n",
    "    df.index = pd.MultiIndex.from_tuples([(x[0], x[1].replace('.smooth', '')) for x in df.index]).set_names(['Inclusion Method', 'Observation Method'])\n",
    "    df.columns = pd.MultiIndex.from_arrays([[\"Vulnerable sites\", \"Vulnerable sites\"], [\"History & Acceptance\", \"Only Acceptance\"]])\n",
    "    display(df)\n",
    "    latex_table = df.style.to_latex(hrules=True, multicol_align=\"c\")\n",
    "    print(latex_table)\n",
    "    with open(f\"res/paper_cookie_{name}.tex\", \"w\") as f:\n",
    "        f.write(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2d084e-b5a4-4c26-8d56-8180c9c3c186",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for name in [\"incs\", \"methods\", \"channels\"]:\n",
    "for name in [\"channels\"]:\n",
    "    df = browser_data[name][[\"Both\", \"Only Cookie\"]]# \"Only Hist\", \"Sum\"]].head(20)\n",
    "    df.index = pd.MultiIndex.from_tuples([(x[0], x[1].replace('.smooth', '')) for x in df.index]).set_names(['Inclusion Method', 'Observation Method'])\n",
    "    df.columns = pd.MultiIndex.from_arrays([[\"Vulnerable sites\", \"Vulnerable sites\"], [\"History & Acceptance\", \"Only Acceptance\"]])\n",
    "    display(df)\n",
    "    df.to_csv(\"res/paper_cookie_channels_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9a7d01-f785-4d17-888c-2b90dee62ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(df.loc[df[('Vulnerable sites', 'Only Acceptance')] > 0])} working channels in chrome acceptance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197829c4-094c-4d06-834c-935502791838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate strange cases?!\n",
    "\n",
    "# Only in history sniffing\n",
    "test = co_hist.loc[co_hist[\"site\"].isin(['tinhte.vn', 'pngwing.com', 'psychologytoday.com', 'sondakika.com', 'sfweekly.com', 'analdin.com', 'gamebanana.com', 'top.gg', 'ct.gov', 'rediff.com', 'united.com', 'cambridge.org'])]\n",
    "# Probably mostly noise? (or only works in completly fresh browser), it is unlikely that the 5+5 test were different, but not impossible\n",
    "\n",
    "# El-blur: \n",
    "# (for visited vs ano: mostly bot detection for some reasons??)\n",
    "# for accepted vs *: blocking cookie banner has auto focus!\n",
    "\n",
    "# securitypolicyviolation\n",
    "# redirect to accept cookie page!\n",
    "\n",
    "with pd.option_context(\"display.max_colwidth\", 500):\n",
    "    display(test.drop_duplicates(subset=[\"browser\", \"site\"]))\n",
    "    #display(co_hist.loc[co_hist[\"observation_methods\"] == \"el-blur\"].drop_duplicates(subset=[\"site\"]))\n",
    "    #display(co_exp.loc[co_exp[\"observation_methods\"] == \"el-securitypolicyviolation\"].drop_duplicates(subset=[\"site\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8115db86-e49a-4091-b742-3f02218f3f67",
   "metadata": {},
   "source": [
    "## Third-parties\n",
    "- third-parties (vs first parties)\n",
    "- links vs requests and stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37465ec-7b70-4f78-8e0d-998633655cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_parties = confs_ano\n",
    "all_parties = all_parties[all_parties.apply(check_5_vals, axis=1)]\n",
    "all_parties = all_parties.explode(\"observation_methods\")\n",
    "all_parties[\"channel\"] = all_parties[\"inc_method\"] + \"-\" + all_parties[\"observation_methods\"]\n",
    "# Appy pM Heuristic again\n",
    "all_parties = all_parties.loc[all_parties.apply(stricter_pm, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b1a783-341a-40a5-97f9-e6d937b7ed83",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(all_parties[\"site\"].nunique())\n",
    "display(all_parties.groupby(\"same_site\")[\"site\"].nunique())\n",
    "display(all_parties.groupby(\"site\")[\"same_site\"].unique().reset_index()[\"same_site\"].apply(sorted).astype(str).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df53cd57-d4a7-488d-b500-4874e61f298c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third-parties that occur often\n",
    "\n",
    "# A lot of cookie syncing\n",
    "# Many popular third-parties\n",
    "third_parties = all_parties.loc[all_parties[\"same_site\"] == False].rename(columns={\"real_site\": \"Third-Party\"}).groupby(\"Third-Party\")[\"site\"].nunique().to_frame().sort_values(\"site\", ascending=False)\n",
    "display(third_parties.head(10))\n",
    "latex_table = third_parties.head(10).style.to_latex()\n",
    "with open(f\"res/paper_third_popular.tex\", \"w\") as f:\n",
    "     f.write(latex_table)\n",
    "display(third_parties.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefa0a83-3132-4bad-a05c-a80b347e6731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs that occur on more than one site\n",
    "all_parties.groupby(\"url\")[\"site\"].nunique().to_frame().sort_values(\"site\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03cd925-582d-46bc-8816-afa2518c5bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how methods change with third-party vs first-party\n",
    "# Channel and co analysis!\n",
    "browser_data = {}\n",
    "\n",
    "for grouping, name in [([\"inc_method\"], \"incs\"), ([\"observation_methods\"], \"methods\"), ([\"inc_method\", \"observation_methods\"], \"channels\")]:\n",
    "    df = all_parties.loc[all_parties[\"observation_methods\"] != \"events-fired-all\"].groupby(grouping).apply(get_uniques, cat=\"site\").apply(pd.Series).sort_values(\"Sum\", ascending=False)\n",
    "    df = df.reset_index().rename(columns={\"inc_method\": \"Inclusion Method\", \"observation_methods\": \"Observation Method\"})\n",
    "    if name == \"incs\":\n",
    "        df[\"Inclusion Method\"] = df[\"Inclusion Method\"].apply(json.loads)\n",
    "        df = df.set_index([\"Inclusion Method\"])\n",
    "    elif name == \"methods\":\n",
    "        df = df.set_index([\"Observation Method\"])\n",
    "    else:\n",
    "        df[\"Inclusion Method\"] = df[\"Inclusion Method\"].apply(json.loads)\n",
    "        df = df.set_index([\"Inclusion Method\", \"Observation Method\"])\n",
    "    df = df.rename(index={\"fetch_response\": \"fetch-response\"})\n",
    "    browser_data[name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaa7397-ce88-4415-8189-2de1c7ee67bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in [\"incs\", \"methods\", \"channels\"]:\n",
    "    df = browser_data[name][[\"Both\", \"Only First\", \"Only Third\", \"Sum\"]].head(20)\n",
    "    display(df)\n",
    "    latex_table = df.style.to_latex()\n",
    "    with open(f\"res/paper_third_{name}.tex\", \"w\") as f:\n",
    "        f.write(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7135ff-2e34-47dd-adc0-012714166742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all channels/methods for percentage of cross-site URLs\n",
    "# Some have very high third-party rates (e.g., perfAPI), others have low third-party rates (e.g., length)\n",
    "conf_t = all_parties\n",
    "conf_t[\"observation_methods\"] = conf_t[\"observation_methods\"].astype(str)\n",
    "conf_t.groupby(\"observation_methods\")[\"same_site\"].agg([\"mean\", \"count\"]).sort_values(\"count\", ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa575a1-ea08-4d62-b2c0-caca16d80972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third-party investigations and stuff:\n",
    "# Firefox:\n",
    "# - State partioning is doing \"weird\" stuff? Load site, load attack site in same tab -> works; load site, load attack site in another tab -> does not work\n",
    "# Chromium:\n",
    "# - Tabs do not matter, they are \"synced\"\n",
    "# Third-party:\n",
    "# - cookies are set for domain (e.g.,: .doubleclick.com) -> thus even though every site has a different subdomain, the information that leaks is only that doubleclick cookies are set already? :(\n",
    "# - for e.g., both perfAPI (doubleclick) and fetch_response (demdex)\n",
    "# PerfAPI:\n",
    "# - when the `timing-allow-origin=*` header allows timing information to leak cross-origin, it also allows to leak the resource size!!\n",
    "# Fetch_response:\n",
    "# - CORS replay \"misconfig\"! \"Access-Control-Allow-Credentials: true, Access-Control-Allow-Methods: GET, POST, OPTIONS, Access-Control-Allow-Origin: http://observer.org:8001\"\n",
    "# - more common then thought?\n",
    "# - redirect to set cookies, if no cookies are set already\n",
    "\n",
    "with pd.option_context(\"display.max_colwidth\", None):\n",
    "    display(all_parties.loc[all_parties[\"observation_methods\"] == \"performanceAPI.smooth\"].drop_duplicates(subset=\"site\").tail(5))\n",
    "    #display(all_parties.loc[all_parties[\"observation_methods\"] == \"fetch_response\"].drop_duplicates(subset=\"site\").head(10))\n",
    "    #display(all_parties.loc[all_parties[\"observation_methods\"] == \"win.performanceAPI.smooth\"].drop_duplicates(subset=\"site\").head(5))\n",
    "    #display(all_parties.loc[all_parties[\"observation_methods\"] == \"width\"].drop_duplicates(subset=\"site\").head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe03634b-b40f-4e42-a8ab-f79c50fee86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Links vs resources/requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7ed56b-4381-4b38-b118-7b2b7e9b317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_dict = {}\n",
    "for row in sites.iterrows():\n",
    "    row = row[1]\n",
    "    site = row[\"site\"]\n",
    "    url_dict = {}\n",
    "    for entry in row[\"urls\"]:\n",
    "        url_dict[entry[\"url\"]] = entry\n",
    "        if entry[\"link\"] and entry[\"request\"]:\n",
    "            print(entry)\n",
    "    site_dict[site] = url_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176095f2-be96-4f1b-99ee-43bb88f5adee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source(row):\n",
    "    site = row[\"site\"]\n",
    "    url = json.loads(row[\"url\"])\n",
    "    entry = site_dict[site][url]\n",
    "    # The later two should be redirects caused by visiting the first one\n",
    "    if url in [f\"https://{site}/\", f\"http://{site}/\", f\"https://www.{site}/\"]:\n",
    "        return \"hompage\"  # \"homepage\"\n",
    "    if entry[\"request\"]:\n",
    "        return \"request\"\n",
    "    if entry[\"link\"]:\n",
    "        return \"link\"\n",
    "    else:\n",
    "        return \"invalid\"\n",
    "all_parties[\"source\"] = all_parties[[\"site\", \"url\"]].apply(get_source, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40174cfc-3e6c-4068-99da-a94d5508d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(all_parties[\"source\"].value_counts())\n",
    "display(all_parties.groupby(\"source\")[\"site\"].nunique())\n",
    "display(all_parties.groupby(\"site\")[\"source\"].unique().reset_index()[\"source\"].apply(sorted).astype(str).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24f299d-7275-4c92-a8e3-9a51998a6cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(all_parties[[\"same_site\", \"source\"]].value_counts().to_frame())\n",
    "all_parties[\"source_site\"] = all_parties[\"same_site\"].apply(str) + \"-\" + all_parties[\"source\"]\n",
    "source_table = all_parties.groupby(\"site\")[\"source_site\"].unique().reset_index()[\"source_site\"].apply(sorted).astype(str).value_counts().to_frame()\n",
    "display(source_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95678737-0481-44c4-ac6c-cbd4ad70ea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(row):\n",
    "    data = row[\"index\"][1:-1]\n",
    "    third_party = []\n",
    "    first_party = []\n",
    "    for entry in data.split(\", \"):\n",
    "        party, source = entry[1:-1].split(\"-\")\n",
    "        if party == \"False\":\n",
    "            third_party.append(source)\n",
    "        else:\n",
    "            first_party.append(source)\n",
    "    return {\"First-Party\": sorted(first_party), \"Third-Party\": sorted(third_party), \"Sites\": row[\"source_site\"]}\n",
    "st = source_table.reset_index().apply(split_data, axis=1, result_type=\"expand\").astype(str)\n",
    "st[\"First-Party\"] = st[\"First-Party\"].apply(lambda x: x.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\"))\n",
    "st[\"Third-Party\"] = st[\"Third-Party\"].apply(lambda x: x.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\"))\n",
    "st = st.set_index([\"First-Party\", \"Third-Party\"])\n",
    "display(st)\n",
    "latex_table = st.head(10).style.to_latex()\n",
    "with open(f\"res/paper_third_source.tex\", \"w\") as f:\n",
    "    f.write(latex_table)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
