{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dde39b1-ba3d-458f-935c-fce25a5a4b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aca3a7-a9c6-4ec2-ab17-8b0cf1f8c375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ac69a9-24ce-45b4-8ad4-b91c7d2e9587",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import check_5_vals, stricter_pm, get_uniques, Conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48ea723-7541-45a4-9330-1b72bf4cce4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PGDATABASE\"] = \"dil_login\"\n",
    "\n",
    "with open(\"../../database.env\") as f:\n",
    "    for line in f:\n",
    "        key, value = line.strip().split('=', 1)\n",
    "        os.environ[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f42ad1a-a09d-41c2-b2de-be434c0ec4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = Conn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1260f24-7fdc-4bec-91c8-6b937c9e1b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples = conn.select(\"SELECT * FROM site\")\n",
    "\n",
    "sites = pd.DataFrame(tuples, columns=[\"id\", \"rank\", \"site\", \"urls\", \"crawl_urls\", \"timeout_crawl\", \"error\", \"error_py\", \"crawled_urls\", \"after_basic\", \"after_trees\", \"after_trees_limit\", \"actual_urls\", \"insertion_time\", \"confirmed_urls\", \"timeout_dyn\", \"finished\", \"login_urls\"]).sort_values(\"rank\")\n",
    "\n",
    "display(sites.head())\n",
    "\n",
    "tuples = conn.select(\"SELECT * FROM accept\")\n",
    "accept = pd.DataFrame(tuples, columns=[\"id\", \"site\", \"rank\", \"browser\", \"version\", \"clicked_count\", \"clicked\", \"locator_count\", \"unique_locators\", \"locators\", \"cookies_before\", \"cookies_after\", \"cookies_new\", \"cookies_removed\", \"cookies_changed\", \"error\", \"insertion_time\"]).sort_values(\"rank\")\n",
    "display(accept.head())\n",
    "\n",
    "more = False\n",
    "if more:\n",
    "    tuples = conn.select(\"SELECT * FROM dyn_conf\")\n",
    "    dyn_conf = pd.DataFrame(tuples, columns=[\"id\", \"browser\", \"version\", \"site\", \"opg_url\", \"url\", \"inc_method\", \"state\", \"run\", \"observation\", \"error\", \"notes\", \"response\", \"insertion_time\"])\n",
    "    display(dyn_conf.head())\n",
    "\n",
    "    tuples = conn.select(\"SELECT * FROM responses\")\n",
    "    resp = pd.DataFrame(tuples, columns=[\"id\", \"site\", \"url\", \"state\", \"req_headers\", \"resp_code\", \"resp_headers\",\n",
    "                                           \"resp_body_hash\", \"resp_body_info\", \"frames\", \"error_text\",\n",
    "                                           \"insertion_time\"])\n",
    "    display(resp.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51826418-f792-4aa8-934b-eba00d924be9",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "## 100 top websites with successful login\n",
    "\n",
    "1. **URL Collection**:\n",
    "    - Visit homepage (https://{site}/) wait until \"load\" (max: 30s) in Chromium\n",
    "    - Extract all HTTP(S) links\n",
    "    - Record all outgoing HTTP(S) requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800d3e01-f8c7-46d0-b720-9efe25d2916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some entries are duplicated (started several times, error in first data collection)\n",
    "# Only keep the last entry\n",
    "sites = sites.sort_values(\"id\")\n",
    "sites = sites.drop_duplicates(subset=\"rank\", keep=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2cd491-88de-47a9-99a3-46c9d627e260",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites[\"crawl_urls\"] = sites[\"crawl_urls\"].apply(lambda x: sorted(x))\n",
    "sites[\"crawled_urls\"] = sites[\"crawled_urls\"].apply(lambda x: sorted(x))\n",
    "sites[\"crawled_any\"] = sites[\"crawled_urls\"].str.len() != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1c2964-8bbe-4a20-b99b-d15d3bd2fc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "print(sites[\"site\"].nunique())\n",
    "with pd.option_context(\"display.max_rows\", 123):\n",
    "    display(sites.sort_values([\"site\", \"rank\"])[[\"rank\", \"site\"]])\n",
    "    \n",
    "# amazon.X; individual instances of amazon not exactly the same\n",
    "\n",
    "# bbc.co.uk redirects to bbc.com, remove bbc.co.uk\n",
    "sites = sites[sites[\"site\"] != \"bbc.co.uk\"]\n",
    "\n",
    "# bit.ly redirects to bitly.com, remove bit.ly\n",
    "sites = sites[sites[\"site\"] != \"bit.ly\"]\n",
    "\n",
    "# google.com and google.com.hk; not exactly the same\n",
    "\n",
    "# wikipedia.org and wikimedia.org; not exactly the same\n",
    "\n",
    "# huffingtonpost.com redirects to huffpost.com; however lower rank; remove huffpost.com\n",
    "sites = sites[sites[\"site\"] != \"huffpost.com\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f288256-fbaa-492d-bcdd-22bef3976a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview of table structure\n",
    "print(\"Overview:\")\n",
    "display(sites.head(2))\n",
    "\n",
    "# Errors on the tested sites (URL + response collection)\n",
    "print(\"Crawled sites:\")\n",
    "display(sites[\"crawled_any\"].value_counts())\n",
    "print(\"Errors on tested sites (crawled-any):\")\n",
    "display(sites[[\"crawled_any\", \"error\"]].apply(lambda x: (x[\"crawled_any\"], x[\"error\"].split(\"\\n\")[0].split(\" at \")[0]), axis=1).value_counts().to_frame())\n",
    "\n",
    "display(sites[\"error_py\"].value_counts().to_frame())\n",
    "\n",
    "# One site not succesfully crawled:  nature.com, could not load landing page\n",
    "display(sites.loc[~sites[\"crawled_any\"]])\n",
    "\n",
    "# One site crashed the infrastructure fandom.com, remove from analysis as data for fandom.com is lost\n",
    "display(sites.loc[sites[\"error_py\"].str.contains(\"ForkPool\")])\n",
    "sites = sites.loc[sites[\"site\"] != \"fandom.com\"]\n",
    "\n",
    "\n",
    "# URLs collected:\n",
    "print(f\"URLs collected on: {sites.loc[sites['urls'].str.len() != 0].shape[0]} sites\")\n",
    "\n",
    "print(f\"URLs attempted to crawl on: {sites.loc[sites['crawl_urls'].str.len() != 0].shape[0]} sites\")\n",
    "\n",
    "# URLs crawled:\n",
    "print(f\"URLs crawled on: {sites.loc[sites['crawled_urls'].str.len() != 0].shape[0]} sites (the ones that are missing here crashed in collect_responses)\")\n",
    "\n",
    "# Same URLs crawled as tried:\n",
    "print(f\"All wanted URLs crawled on: {sites.loc[(sites['crawled_urls'] == sites['crawl_urls']) & (sites['crawled_urls'].str.len() != 0)].shape[0]} sites (either timeout or othe issue occured, e.g., crash in collect_responses)\")\n",
    "\n",
    "\n",
    "\n",
    "# Limit to crawled any sites\n",
    "sites_crawled = sites.loc[sites[\"crawled_any\"]]\n",
    "print(\"Timeouts:\")\n",
    "display(sites_crawled[[\"timeout_crawl\"]].value_counts().to_frame())\n",
    "display(sites_crawled[[\"timeout_dyn\"]].value_counts().to_frame())\n",
    "display(sites_crawled[[\"timeout_crawl\", \"timeout_dyn\"]].value_counts().to_frame())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f77dd3-72e5-4ea3-b914-97fa14faa614",
   "metadata": {},
   "source": [
    "2. **Response Collection**:\n",
    "    - Open two chromium instances\n",
    "    - visited homepage state **_visited_l** + login state **_login** (provided externally)\n",
    "    - Visit URLs in all states (2)\n",
    "        - on every URL wait until \"load\" (max: 30s) (top-level request)\n",
    "        - max 1000 URLs (if more than 1000 exist, random selection of all recorded URLs)\n",
    "        - max 1 hour\n",
    "        - Record traffic/responses (with playwright; does not record everything for errors and similar; other option would be HAR or proxy?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae31711-7869-42e2-b2fc-214c9c3f48cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(row):\n",
    "    ll = row[\"urls\"]\n",
    "    links = []\n",
    "    requests = []\n",
    "    total = len(ll)\n",
    "    for l in ll:\n",
    "        if l[\"link\"]:\n",
    "            links.append(l)\n",
    "        if l[\"request\"]:\n",
    "            requests.append(l)\n",
    "    return {\"Links\": len(links), \"Requests\": len(requests), \"Total\": total}\n",
    "\n",
    "print(\"URLs collected stats:\")\n",
    "display(sites_crawled.apply(count, result_type=\"expand\", axis=1).describe())\n",
    "display(sites_crawled.apply(count, result_type=\"expand\", axis=1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff7ecf9-e1ed-411b-85df-5822660f0a46",
   "metadata": {},
   "source": [
    "3. **Pruning**:\n",
    "    - Get all traffic data for all crawled URLs\n",
    "    - Fit response data to trees:\n",
    "        - Status-Code\n",
    "        - smoothed (Security)-Headers: \"content-type\", \"x-frame-options\", \"location\", \"content-disposition\", \"x-content-type-options\", \"cross-origin-opener-policy\", \"cross-origin-resource-policy\", \"content-security-policy\"\n",
    "        - body type: e.g., HTML, img, ... (inferred with `file` command)\n",
    "    - Basic pruning: only keep URLs that have at least one attribute with more than one recorded value\n",
    "    - Advanced pruning:\n",
    "        - All Chromium and Firefox trees\n",
    "        - Predict the outcome of every tree for every remaining URL-state pair\n",
    "        - For every tree with at least two different predictions for a URL -> add URL-inclusion method to set of to confirm URLs\n",
    "            - special cases for some trees (e.g., img-height):\n",
    "                - even if all predictions are the same, they might be distinguishable (artifact of the smoothing)\n",
    "                - if all predictions are positive (e.g, height=50), compare other property (e.g., bodyhash) and if that property differs -> add to set\n",
    "            - example:     `urls = {\"img\": {\"https://google.com/\": \"cfw\"}, \"https://google.com/search/\": \"c\"}, \"iframe\": {\"https://google.com/\": \"f\"}}`; every inc-url pair is tested in both browsers regardless of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c0fb9e-2855-49cb-8856-a4b22c4a255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sites crawled\n",
    "sc = sites_crawled\n",
    "def get_urls(dat):\n",
    "    url_set = set()\n",
    "    url_list = []\n",
    "    for inc, entry in dat.items():\n",
    "        for url, browser in entry.items():\n",
    "            url_list.append(url)\n",
    "            url_set.add(url)\n",
    "    return url_list, url_set\n",
    "\n",
    "def count_pruning(row):\n",
    "    pc = row[\"crawl_urls\"]\n",
    "    c = row[\"crawled_urls\"]\n",
    "    ab = row[\"after_basic\"]\n",
    "    at = row[\"after_trees\"]\n",
    "    atl = row[\"after_trees_limit\"]\n",
    "    act = row[\"actual_urls\"]\n",
    "    ul_at, us_at = get_urls(at)\n",
    "    ul_atl, us_atl = get_urls(atl)\n",
    "    ul_act, us_act = get_urls(act)\n",
    "                \n",
    "    return {\"crawl_urls\": len(pc), \"crawled_urls\": len(c), \"after basic\": len(ab), \"after trees (total inc-url pairs)\": len(ul_at), \"after trees (unique urls)\": len(us_at), \"after trees limit (total inc-url pairs)\": len(ul_atl), \"after trees limit (unique urls)\": len(us_atl), \"actual URLs\": len(ul_act), \"actual URLs (unique)\": len(us_act)}\n",
    "# The data describes it without browsers!\n",
    "print(\"Pruning stats:\")\n",
    "display(sc[[\"crawl_urls\", \"crawled_urls\", \"after_basic\", \"after_trees\", \"after_trees_limit\", \"actual_urls\"]].apply(count_pruning, axis=1, result_type=\"expand\").describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0685dd5-cd86-4f51-a940-af80863b6564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which inclusion methods are predicted?\n",
    "# Sites/URLs\n",
    "def collect_incs(row):\n",
    "    row = row[\"after_trees\"]\n",
    "    res = {\"any\": {}}\n",
    "    for inc in row.keys():\n",
    "        for url, browser_str in row[inc].items():\n",
    "            entry = res[\"any\"].get(inc, 0)\n",
    "            entry += 1\n",
    "            res[\"any\"][inc] = entry\n",
    "    return res\n",
    "met = sc[[\"after_trees\"]].apply(collect_incs, axis=1, result_type=\"expand\")\n",
    "met_any = pd.json_normalize(met[\"any\"]).agg([\"count\", \"sum\"]).T\n",
    "met_any[[\"count\", \"sum\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a7cc1f-cb8a-4e3b-a3e6-a1f37ad9e1ea",
   "metadata": {},
   "source": [
    "4. **\"Dynamic confirmation**:\n",
    "    - Test all remaining inclusion_method-url-browser pairs\n",
    "        - max 25 URLs for one inclusion method\n",
    "        - max 3h\n",
    "    - Test all possible states (regardless of whether the prediction was only for one state-pair)\n",
    "    - Prepare states: \n",
    "        - Same as in **response collection**\n",
    "        - Additionally for Firefox\n",
    "    - For every inc method:\n",
    "        - For every URL:\n",
    "            - For every browser; If browser should be tested:\n",
    "                - For every state:\n",
    "                    - wait 1s\n",
    "                    - visit `http://observer.org/opg/<inc>/?url=<url>`\n",
    "                    - wait until \"networkidle\", max: 30s; for window.open wait for \"networkidle\" or \"domcontentloaded\" of the new window\n",
    "                    - wait another 750ms (2000ms)\n",
    "                    - extract observations\n",
    "                    - (record responses)\n",
    "                - If observations for every state are the same -> remove browser from to_test list\n",
    "         - Repeat up to 5 times\n",
    "     - Get confirmed distinguishable pairs:\n",
    "         - 5 times different observations for one observation method -> confirmed browser-inc_method-url-state_a-state_b(-observation_method) pair\n",
    "         - additional sanity checking: \n",
    "             - the same observation is not allowed to be present in both states (e.g., random frame counts: [(0, 1), (0, 1), (1, 0), (1, 0), (0, 1)] -> not a confirmed pair)\n",
    "             - additional constraints for some methods: \n",
    "                 - e.g., custom code for postMessage, frame_count\n",
    "                 - heuristic: at least one value should occur two times for the same state?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a595c8fc-a5f6-4142-91bc-db47be910186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timout of dynamic sites\n",
    "sites_dyn = sc.loc[sc[\"actual_urls\"] != {}]\n",
    "display(len(sites_dyn))\n",
    "display(sites_dyn[\"timeout_dyn\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cb63fa-8a4f-4540-9122-23748ae77c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early abort stats\n",
    "dyn_conf_run = pd.DataFrame(conn.select(\"SELECT run, COUNT(id) from dyn_conf GROUP BY run\"))\n",
    "dyn_conf_run.loc[\"sum\"] = dyn_conf_run.sum()\n",
    "display(dyn_conf_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86379b8e-0953-465d-afc3-a7ccae36f902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total time taken\n",
    "sc[\"insertion_time\"].max() - sc[\"insertion_time\"].min() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9bd961-311b-4de3-a2c9-10f6c210da63",
   "metadata": {},
   "source": [
    "## Login detection preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451be7af-1462-4faa-8d8e-d80462134fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit to first 100 unique sites!\n",
    "sc = sc.iloc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296b19cf-8fca-4e09-b472-2892cbff2904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data (one entry for every confirmed URL)\n",
    "conf = sc.loc[sc[\"confirmed_urls\"].str.len() != 0]\n",
    "confs_raw = pd.DataFrame()\n",
    "for row in conf[[\"confirmed_urls\", \"site\", \"rank\"]].iterrows():\n",
    "    row = row[1]\n",
    "    site = row[\"site\"]\n",
    "    rank = row[\"rank\"]\n",
    "    for state, df in row[\"confirmed_urls\"].items():\n",
    "        new = pd.DataFrame.from_dict(df)\n",
    "        new[\"site\"] = site\n",
    "        new[\"rank\"] = rank\n",
    "        new[\"state\"] = state\n",
    "        new = new.rename(columns={\"0\": \"observation_methods\"})\n",
    "        confs_raw = pd.concat([confs_raw, new])\n",
    "display(confs_raw.head())\n",
    "\n",
    "confs_raw[\"observation_methods\"] = confs_raw[\"observation_methods\"].apply(sorted)\n",
    "confs_old = confs_raw.copy()\n",
    "from publicsuffix2 import get_sld\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "confs_raw[\"real_site\"] = confs_raw[\"url\"].apply(lambda x: get_sld(urlparse(json.loads(x)).hostname))\n",
    "confs_raw[\"same_site\"] = confs_raw[\"site\"] == confs_raw[\"real_site\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9a448b-b220-4552-a85c-fa48634767b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_parties = confs_raw\n",
    "all_parties = all_parties[all_parties.apply(check_5_vals, axis=1)]\n",
    "all_parties = all_parties.explode(\"observation_methods\")\n",
    "all_parties[\"channel\"] = all_parties[\"inc_method\"] + \"-\" + all_parties[\"observation_methods\"]\n",
    "# Appy pM Heuristic again\n",
    "all_parties = all_parties.loc[all_parties.apply(stricter_pm, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cb6c0c-7874-47ec-8881-50843fb3b335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add lax variant of same-site -> same-party\n",
    "# first: top-level redirects e.g., blogspot.com == blogger.com\n",
    "# second: same-party/account used: e.g., google.com == youtube.com\n",
    "\n",
    "with pd.option_context(\"display.max_colwidth\", None):\n",
    "    display(all_parties.loc[all_parties[\"site\"] != all_parties[\"real_site\"]].groupby([\"rank\", \"site\"])[\"real_site\"].unique().to_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ee9c5a-7ff5-47db-a6b1-4b4b4c02b8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "allow_dict = {\n",
    "    \"bbc.com\": [\"bbc.co.uk\"],\n",
    "    \"blogspot.com\": [\"blogger.com\"],\n",
    "    \"businessinsider.com\": [\"businessinsider.de\"],\n",
    "    \"goo.gl\": [\"google.com\", \"youtube.com\"],\n",
    "    \"google.com.hk\": [\"google.com\"],\n",
    "    \"huffingtonpost.com\": [\"huffpost.com\"],\n",
    "    \"pinterest.com\": [\"pinterest.de\"],\n",
    "    \"steampowered.com\": [\"steamcommunity.com\"],\n",
    "    \"steamcomunity.com\": [\"steampowered.com\"],\n",
    "    \"techcrunch.com\": [\"yahoo.com\"],\n",
    "    \"yelp.com\": [\"yelp.de\"],\n",
    "    \"youtube.com\": [\"google.com\"],\n",
    "    \"zoho.com\": [\"zoho.eu\"],\n",
    "    \"suara.com\": [\"zonautara.com\"],\n",
    "    \"theverge.com\": [\"voxmedia.com\"]\n",
    "}\n",
    "# bbc.com allow bbc.co.uk\n",
    "# blogspot.com allow blogger.com\n",
    "# businessinsider.com allow businessinsider.de\n",
    "# goo.gl allow google.com, youtube.com\n",
    "# google.com.hk allow google.com\n",
    "# huffingtonpost.com allow huffpost.com\n",
    "# pinterest.com allow pinterest.de\n",
    "# steampowered.com allow steamcommunity.com\n",
    "# techcrunch.com allow yahoo.com\n",
    "# yelp.com allow yelp.de\n",
    "# youtube.com allow google.com\n",
    "\n",
    "def get_first_party(site):\n",
    "    sites = allow_dict.get(site, []).copy()\n",
    "    sites.append(site)\n",
    "    return sites\n",
    "\n",
    "def check_party(row):\n",
    "    if row[\"real_site\"] in row[\"first_party\"]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "confs_raw[\"first_party\"] = confs_raw[\"site\"].apply(get_first_party)\n",
    "display(confs_raw[\"first_party\"].astype(str).value_counts())\n",
    "confs_raw[\"same_party\"] = confs_raw.apply(check_party, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed86d859-eb82-4424-9a91-daa8c8b6af75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to stricter heuristic for analysis!\n",
    "confs_raw = confs_raw[confs_raw.apply(check_5_vals, axis=1)]\n",
    "# Use even stricter heuristic for pMs:\n",
    "# one state is only allowed to have a maximum of one observation, this leads to some FNs, but should remove all FPs\n",
    "confs_raw = confs_raw.loc[confs_raw.apply(stricter_pm, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea18953-475d-43ab-b537-ced9b26e963f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"All considered: {sc['site'].nunique()}\")\n",
    "print(f\"All vulnerable: {confs_raw['site'].nunique()}\")\n",
    "print(f\"Same-site only: {confs_raw.loc[confs_raw['same_site']]['site'].nunique()}\")\n",
    "print(f\"Same-party only: {confs_raw.loc[confs_raw['same_party']]['site'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74f66c8-0c7e-4f33-841a-9de878571ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sites not making the stricter heuristics\n",
    "set(confs_old[\"site\"].unique()) ^ set(confs_raw[\"site\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9c128f-e518-46cc-a296-6b65f1292abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sites only vulnerable in third-party context\n",
    "only_third = set(confs_raw[\"site\"].unique()) ^ set(confs_raw.loc[confs_raw['same_party']]['site'].unique())\n",
    "print(only_third)\n",
    "confs_raw.loc[confs_raw[\"site\"].isin(only_third)][[\"site\", \"real_site\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd4c79d-9871-4b81-9295-ab44adeb55ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sites not vulnerable at all\n",
    "sc.loc[sc[\"confirmed_urls\"].str.len() == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cba904-ac0b-46ef-9162-4dda4b5816e0",
   "metadata": {},
   "source": [
    "## Login detection results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac2b225-2c83-41ae-ac2f-36e28aa266d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "confs = confs_raw\n",
    "# Confs same-party only\n",
    "confs = confs.loc[confs[\"same_party\"] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a293388b-f4d0-491d-a688-fbe18fafedc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique sites\", confs[\"site\"].nunique())\n",
    "print(confs[\"observation_methods\"].value_counts().to_frame().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae00a47e-b380-413d-b983-b93e2ed32304",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(confs[\"url\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb58ca0f-c285-4e23-abe1-ce0b7e184414",
   "metadata": {},
   "outputs": [],
   "source": [
    "confs[\"channel\"] = confs[\"inc_method\"] + \"-\" +  confs[\"observation_methods\"].apply(str)\n",
    "display(confs[\"site\"].nunique())\n",
    "display(confs.groupby([\"site\"])[\"browser\"].unique().apply(sorted).astype(str).to_frame().value_counts().to_frame())\n",
    "display(confs.groupby([\"browser\", \"state\"])[\"site\"].nunique().to_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3498608-a51b-4817-ab6d-111c625a96e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the observation methods to have one row for every observation method\n",
    "c_exp = confs.explode(\"observation_methods\")\n",
    "c_exp[\"channel\"] = c_exp[\"inc_method\"] + \"-\" + c_exp[\"observation_methods\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27ef105-3609-466f-bfa7-1554e549bb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average URLs/site\n",
    "display(c_exp.groupby(\"site\")[\"url\"].nunique().to_frame().describe())\n",
    "# Average inc-url-pairs/site\n",
    "display(c_exp.groupby(\"site\")[\"opg_url\"].nunique().to_frame().describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ffe1d5-8a55-4cab-9280-dcf29b4e91f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of vulnerable sites\n",
    "c_exp[\"site\"].nunique()/len(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2702571-bd6c-4655-ac24-769d7af324b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fancy tables with Sites both browser, only chromium, only firefox, (sorted by sum)\n",
    "browser_data = {}\n",
    "\n",
    "for grouping, name in [([\"inc_method\"], \"incs\"), ([\"observation_methods\"], \"methods\"), ([\"inc_method\", \"observation_methods\"], \"channels\")]:\n",
    "    df = c_exp.loc[c_exp[\"observation_methods\"] != \"events-fired-all\"].groupby(grouping).apply(get_uniques).apply(pd.Series).sort_values(\"Sum\", ascending=False)\n",
    "    df = df.reset_index().rename(columns={\"inc_method\": \"Inclusion Method\", \"observation_methods\": \"Observation Method\"})\n",
    "    if name == \"incs\":\n",
    "        df[\"Inclusion Method\"] = df[\"Inclusion Method\"].apply(json.loads)\n",
    "        df = df.set_index([\"Inclusion Method\"])\n",
    "    elif name == \"methods\":\n",
    "        df = df.set_index([\"Observation Method\"])\n",
    "    else:\n",
    "        df[\"Inclusion Method\"] = df[\"Inclusion Method\"].apply(json.loads)\n",
    "        df = df.set_index([\"Inclusion Method\", \"Observation Method\"])\n",
    "    df = df.rename(index={\"fetch_response\": \"fetch-response\"})\n",
    "    browser_data[name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaa3a12-f6e2-4039-997a-34e90ac92a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for name in [\"incs\", \"methods\", \"channels\"]:\n",
    "for name in [\"channels\"]:\n",
    "    df = browser_data[name][[\"Both\", \"Only C\", \"Only FF\", \"Sum\"]].head(20)\n",
    "    df.index = pd.MultiIndex.from_tuples([(x[0], x[1].replace('.smooth', '')) for x in df.index]).set_names(['Inclusion Method', 'Observation Method'])\n",
    "    \n",
    "    df.columns = pd.MultiIndex.from_arrays([[\"Vulnerable sites\", \"Vulnerable sites\", \"Vulnerable sites\", \"Vulnerable sites\"], [\"Both\", \"Only Chromium\", \"Only Firefox\", \"Sum\"]])\n",
    "    display(df)\n",
    "    latex_table = df.style.to_latex(hrules=True, multicol_align=\"c\")\n",
    "    with open(f\"res/paper_login_{name}.tex\", \"w\") as f:\n",
    "        f.write(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f470117-6130-4ac9-a33e-25638d52f07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for name in [\"incs\", \"methods\", \"channels\"]:\n",
    "for name in [\"channels\"]:\n",
    "    df = browser_data[name][[\"Both\", \"Only C\", \"Only FF\", \"Sum\"]]\n",
    "    print(len(df))\n",
    "    df.index = pd.MultiIndex.from_tuples([(x[0], x[1].replace('.smooth', '')) for x in df.index]).set_names(['Inclusion Method', 'Observation Method'])\n",
    "    \n",
    "    df.columns = pd.MultiIndex.from_arrays([[\"Vulnerable sites\", \"Vulnerable sites\", \"Vulnerable sites\", \"Vulnerable sites\"], [\"Both\", \"Only Chromium\", \"Only Firefox\", \"Sum\"]])\n",
    "    display(df)\n",
    "    latex_table = df.style.to_latex(hrules=True, multicol_align=\"c\")\n",
    "    with open(f\"res/paper_login_{name}_full.tex\", \"w\") as f:\n",
    "        f.write(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0a08a7-b766-493e-bf37-f0be07a2372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"res/paper_login_channels_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f31cc1-6b4b-49eb-8791-8ae495cf4a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(df.loc[df[('Vulnerable sites', 'Only Chromium')] > 0])} working channels in chrome login\")\n",
    "print(f\"{len(df.loc[df[('Vulnerable sites', 'Only Firefox')] > 0])} working channels in firefox login\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b452e2-afa8-4672-b64f-d9bda75a0b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context(\"display.max_rows\", 84):\n",
    "    display(browser_data[\"channels\"][[\"Both\", \"Only C\", \"Only FF\", \"Sum\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64e04e3-93fd-4ef7-8aee-acf51f89f62b",
   "metadata": {},
   "source": [
    "### Login vs non-login URLs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7ed56b-4381-4b38-b118-7b2b7e9b317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_dict = {}\n",
    "for row in sites.iterrows():\n",
    "    row = row[1]\n",
    "    site = row[\"site\"]\n",
    "    url_dict = {}\n",
    "    for entry in row[\"urls\"]:\n",
    "        entry[\"visited\"] = True\n",
    "        entry[\"login\"] = False\n",
    "        url_dict[entry[\"url\"]] = entry\n",
    "        if entry[\"link\"] and entry[\"request\"]:\n",
    "            print(entry)\n",
    "    for entry in row[\"login_urls\"]:\n",
    "        entry[\"login\"] = True\n",
    "        if url_dict.get(entry[\"url\"], None):\n",
    "            entry[\"visited\"] = True\n",
    "        else:\n",
    "            entry[\"visited\"] = False\n",
    "        url_dict[entry[\"url\"]] = entry    \n",
    "    site_dict[site] = url_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176095f2-be96-4f1b-99ee-43bb88f5adee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source(row):\n",
    "    site = row[\"site\"]\n",
    "    url = json.loads(row[\"url\"])\n",
    "    entry = site_dict[site][url]\n",
    "    # The later two should be redirects caused by visiting the first one\n",
    "    if url in [f\"https://{site}/\", f\"http://{site}/\", f\"https://www.{site}/\"]:\n",
    "        return \"hompage\"  # \"homepage\"\n",
    "    if entry[\"request\"]:\n",
    "        return \"request\"\n",
    "    if entry[\"link\"]:\n",
    "        return \"link\"\n",
    "    else:\n",
    "        return \"invalid\"\n",
    "confs[\"source\"] = confs[[\"site\", \"url\"]].apply(get_source, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1704c554-1269-45e5-8220-9b0b043c78b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(row):\n",
    "    site = row[\"site\"]\n",
    "    url = json.loads(row[\"url\"])\n",
    "    entry = site_dict[site][url]\n",
    "    login = entry[\"login\"] \n",
    "    visited = entry[\"visited\"]\n",
    "    if login and visited:\n",
    "        return \"login and visited\"\n",
    "    elif login:\n",
    "        return \"login\"\n",
    "    elif visited:\n",
    "        return \"visited\"\n",
    "    else:\n",
    "        raise Exception(\"Invalid!\")\n",
    "confs[\"state_source\"] = confs[[\"site\", \"url\"]].apply(get_state, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0192b0d0-e17d-4bc3-9240-98fa83fcc8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(confs[\"state_source\"].value_counts())\n",
    "display(confs.groupby(\"state_source\")[\"site\"].nunique())\n",
    "display(confs.groupby(\"site\")[\"state_source\"].unique().reset_index()[\"state_source\"].apply(sorted).astype(str).value_counts())\n",
    "\n",
    "display(confs[[\"same_site\", \"state_source\"]].value_counts().to_frame())\n",
    "confs[\"state_source_site\"] = confs[\"same_party\"].apply(str) + \"-\" + confs[\"state_source\"]\n",
    "source_table = confs.groupby(\"site\")[\"state_source_site\"].unique().reset_index()[\"state_source_site\"].apply(sorted).astype(str).value_counts().to_frame()\n",
    "display(source_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b28caf-3340-4054-94b4-d28c46fd84e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(confs.groupby(\"site\")[\"state_source\"].unique().reset_index()[\"state_source\"].apply(sorted).astype(str).value_counts())\n",
    "login_sites = confs.groupby(\"site\")[\"state_source\"].unique().astype(str).to_frame()\n",
    "display(login_sites.head())\n",
    "login_sites = login_sites.loc[login_sites[\"state_source\"] == \"['login']\"].index\n",
    "login_urls = confs.loc[confs[\"site\"].isin(login_sites)][[\"site\", \"url\"]].drop_duplicates()\n",
    "\n",
    "def left_align(df):\n",
    "    left_aligned_df = df.style.set_properties(**{'text-align': 'left'})\n",
    "    left_aligned_df = left_aligned_df.set_table_styles(\n",
    "        [dict(selector='th', props=[('text-align', 'left')])]\n",
    "    )\n",
    "    return left_aligned_df\n",
    "\n",
    "# Sites only vulnerable to Login URLs, check that URLs are not session URLs\n",
    "# No session URLs\n",
    "with pd.option_context(\"display.max_colwidth\", None):\n",
    "    display(left_align(login_urls.reset_index()))\n",
    "    \n",
    "# All vulnerable URLs found in login mode\n",
    "# Some might be session URLs, but does not matter as other vulnerable URLs exist on the site\n",
    "with pd.option_context(\"display.max_colwidth\", None):\n",
    "    display(left_align(confs.loc[confs[\"state_source\"] == \"login\"][[\"site\", \"url\"]].drop_duplicates().reset_index()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cff2996-2336-464f-96e8-091673405601",
   "metadata": {},
   "source": [
    "### Link vs requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40174cfc-3e6c-4068-99da-a94d5508d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(confs[\"source\"].value_counts())\n",
    "display(confs.groupby(\"source\")[\"site\"].nunique())\n",
    "display(confs.groupby(\"site\")[\"source\"].unique().reset_index()[\"source\"].apply(sorted).astype(str).value_counts())\n",
    "\n",
    "display(confs[[\"same_site\", \"source\"]].value_counts().to_frame())\n",
    "confs[\"source_site\"] = confs[\"same_site\"].apply(str) + \"-\" + confs[\"source\"]\n",
    "source_table = confs.groupby(\"site\")[\"source_site\"].unique().reset_index()[\"source_site\"].apply(sorted).astype(str).value_counts().to_frame()\n",
    "display(source_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95678737-0481-44c4-ac6c-cbd4ad70ea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(row):\n",
    "    data = row[\"index\"][1:-1]\n",
    "    third_party = []\n",
    "    first_party = []\n",
    "    for entry in data.split(\", \"):\n",
    "        party, source = entry[1:-1].split(\"-\")\n",
    "        if party == \"False\":\n",
    "            third_party.append(source)\n",
    "        else:\n",
    "            first_party.append(source)\n",
    "    return {\"First-Party\": sorted(first_party), \"Third-Party\": sorted(third_party), \"Sites\": row[\"source_site\"]}\n",
    "st = source_table.reset_index().apply(split_data, axis=1, result_type=\"expand\").astype(str)\n",
    "st[\"First-Party\"] = st[\"First-Party\"].apply(lambda x: x.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\"))\n",
    "st[\"Third-Party\"] = st[\"Third-Party\"].apply(lambda x: x.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\"))\n",
    "st = st.set_index([\"First-Party\", \"Third-Party\"])\n",
    "display(st)\n",
    "latex_table = st.head(10).style.to_latex()\n",
    "with open(f\"res/paper_third_login_source.tex\", \"w\") as f:\n",
    "    f.write(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702bef92-d2ba-4306-8a01-34ca3412e0e6",
   "metadata": {},
   "source": [
    "## Third-parties\n",
    "- third-parties (vs first parties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91c71d7-b540-4749-969c-72166368994c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_parties = confs_raw.explode(\"observation_methods\")\n",
    "all_parties[\"channel\"] = all_parties[\"inc_method\"] + \"-\" + all_parties[\"observation_methods\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279f10b4-be7d-475d-8295-14790a24cd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(all_parties[\"site\"].nunique())\n",
    "display(all_parties.groupby(\"same_site\")[\"site\"].nunique())\n",
    "display(all_parties.groupby(\"site\")[\"same_site\"].unique().reset_index()[\"same_site\"].apply(sorted).astype(str).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19558c86-444e-450c-ae3d-6c231c228c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third-parties that occur often\n",
    "\n",
    "# A lot of cookie syncing\n",
    "# Many popular third-parties\n",
    "third_parties = all_parties.loc[all_parties[\"same_site\"] == False].rename(columns={\"real_site\": \"Third-Party\"}).groupby(\"Third-Party\")[\"site\"].nunique().to_frame().sort_values(\"site\", ascending=False)\n",
    "display(third_parties.head(10))\n",
    "latex_table = third_parties.head(10).style.to_latex()\n",
    "with open(f\"res/paper_third_login_popular.tex\", \"w\") as f:\n",
    "     f.write(latex_table)\n",
    "display(third_parties.describe())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
